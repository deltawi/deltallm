# ProxyLLM Pricing Configuration
# Version: 1.0
# 
# This file contains manual pricing configuration for all supported models.
# Prices are in USD per unit (token, image, character, minute, etc.)
#
# To reload: Changes are automatically detected if hot_reload is enabled,
# or restart the server.

version: "1.0"

pricing:
  # ============================================================================
  # OpenAI GPT-4o Models
  # ============================================================================
  gpt-4o:
    mode: chat
    input_cost_per_token: 0.0000025    # $2.50 / 1M tokens
    output_cost_per_token: 0.00001     # $10.00 / 1M tokens
    max_tokens: 128000
    max_output_tokens: 16384

  gpt-4o-2024-11-20:
    mode: chat
    input_cost_per_token: 0.0000025
    output_cost_per_token: 0.00001
    max_tokens: 128000
    max_output_tokens: 16384

  gpt-4o-2024-08-06:
    mode: chat
    input_cost_per_token: 0.0000025
    output_cost_per_token: 0.00001
    max_tokens: 128000
    max_output_tokens: 16384

  gpt-4o-mini:
    mode: chat
    input_cost_per_token: 0.00000015   # $0.15 / 1M tokens
    output_cost_per_token: 0.0000006   # $0.60 / 1M tokens
    max_tokens: 128000
    max_output_tokens: 16384

  gpt-4o-mini-2024-07-18:
    mode: chat
    input_cost_per_token: 0.00000015
    output_cost_per_token: 0.0000006
    max_tokens: 128000
    max_output_tokens: 16384

  # ============================================================================
  # OpenAI GPT-4 Turbo Models
  # ============================================================================
  gpt-4-turbo:
    mode: chat
    input_cost_per_token: 0.00001      # $10 / 1M tokens
    output_cost_per_token: 0.00003     # $30 / 1M tokens
    max_tokens: 128000
    max_output_tokens: 4096

  gpt-4-turbo-2024-04-09:
    mode: chat
    input_cost_per_token: 0.00001
    output_cost_per_token: 0.00003
    max_tokens: 128000
    max_output_tokens: 4096

  gpt-4-turbo-preview:
    mode: chat
    input_cost_per_token: 0.00001
    output_cost_per_token: 0.00003
    max_tokens: 128000
    max_output_tokens: 4096

  # ============================================================================
  # OpenAI GPT-4 Models
  # ============================================================================
  gpt-4:
    mode: chat
    input_cost_per_token: 0.00003      # $30 / 1M tokens
    output_cost_per_token: 0.00006     # $60 / 1M tokens
    max_tokens: 8192
    max_output_tokens: 8192

  gpt-4-0613:
    mode: chat
    input_cost_per_token: 0.00003
    output_cost_per_token: 0.00006
    max_tokens: 8192
    max_output_tokens: 8192

  gpt-4-32k:
    mode: chat
    input_cost_per_token: 0.00006      # $60 / 1M tokens
    output_cost_per_token: 0.00012     # $120 / 1M tokens
    max_tokens: 32768
    max_output_tokens: 32768

  gpt-4-32k-0613:
    mode: chat
    input_cost_per_token: 0.00006
    output_cost_per_token: 0.00012
    max_tokens: 32768
    max_output_tokens: 32768

  # ============================================================================
  # OpenAI GPT-3.5 Turbo Models
  # ============================================================================
  gpt-3.5-turbo:
    mode: chat
    input_cost_per_token: 0.0000005    # $0.50 / 1M tokens
    output_cost_per_token: 0.0000015   # $1.50 / 1M tokens
    max_tokens: 16385
    max_output_tokens: 4096

  gpt-3.5-turbo-0125:
    mode: chat
    input_cost_per_token: 0.0000005
    output_cost_per_token: 0.0000015
    max_tokens: 16385
    max_output_tokens: 4096

  gpt-3.5-turbo-1106:
    mode: chat
    input_cost_per_token: 0.000001     # $1 / 1M tokens
    output_cost_per_token: 0.000002    # $2 / 1M tokens
    max_tokens: 16385
    max_output_tokens: 4096

  gpt-3.5-turbo-16k:
    mode: chat
    input_cost_per_token: 0.000003     # $3 / 1M tokens
    output_cost_per_token: 0.000004    # $4 / 1M tokens
    max_tokens: 16384
    max_output_tokens: 16384

  gpt-3.5-turbo-instruct:
    mode: chat
    input_cost_per_token: 0.0000015    # $1.50 / 1M tokens
    output_cost_per_token: 0.000002    # $2 / 1M tokens
    max_tokens: 4096
    max_output_tokens: 4096

  # ============================================================================
  # OpenAI o1 Models (Reasoning)
  # ============================================================================
  o1:
    mode: chat
    input_cost_per_token: 0.000015     # $15 / 1M tokens
    output_cost_per_token: 0.00006     # $60 / 1M tokens
    max_tokens: 200000
    max_output_tokens: 100000

  o1-2024-12-17:
    mode: chat
    input_cost_per_token: 0.000015
    output_cost_per_token: 0.00006
    max_tokens: 200000
    max_output_tokens: 100000

  o1-preview:
    mode: chat
    input_cost_per_token: 0.000015
    output_cost_per_token: 0.00006
    max_tokens: 128000
    max_output_tokens: 32768

  o1-mini:
    mode: chat
    input_cost_per_token: 0.000003     # $3 / 1M tokens
    output_cost_per_token: 0.000012    # $12 / 1M tokens
    max_tokens: 128000
    max_output_tokens: 65536

  # ============================================================================
  # Anthropic Claude 3.5 Models
  # ============================================================================
  claude-3-5-sonnet-20241022:
    mode: chat
    input_cost_per_token: 0.000003     # $3 / 1M tokens
    output_cost_per_token: 0.000015    # $15 / 1M tokens
    cache_creation_input_token_cost: 0.00000375   # $3.75 / 1M
    cache_read_input_token_cost: 0.0000003        # $0.30 / 1M
    max_tokens: 200000
    max_output_tokens: 8192

  claude-3-5-sonnet-20240620:
    mode: chat
    input_cost_per_token: 0.000003
    output_cost_per_token: 0.000015
    cache_creation_input_token_cost: 0.00000375
    cache_read_input_token_cost: 0.0000003
    max_tokens: 200000
    max_output_tokens: 8192

  claude-3-5-haiku-20241022:
    mode: chat
    input_cost_per_token: 0.0000008    # $0.80 / 1M tokens
    output_cost_per_token: 0.000004    # $4 / 1M tokens
    max_tokens: 200000
    max_output_tokens: 8192

  # ============================================================================
  # Anthropic Claude 3 Models
  # ============================================================================
  claude-3-opus-20240229:
    mode: chat
    input_cost_per_token: 0.000015     # $15 / 1M tokens
    output_cost_per_token: 0.000075    # $75 / 1M tokens
    cache_creation_input_token_cost: 0.00001875
    cache_read_input_token_cost: 0.0000015
    max_tokens: 200000
    max_output_tokens: 4096

  claude-3-sonnet-20240229:
    mode: chat
    input_cost_per_token: 0.000003
    output_cost_per_token: 0.000015
    max_tokens: 200000
    max_output_tokens: 4096

  claude-3-haiku-20240307:
    mode: chat
    input_cost_per_token: 0.00000025   # $0.25 / 1M tokens
    output_cost_per_token: 0.00000125  # $1.25 / 1M tokens
    max_tokens: 200000
    max_output_tokens: 4096

  # ============================================================================
  # Cohere Models
  # ============================================================================
  command-r:
    mode: chat
    input_cost_per_token: 0.0000005    # $0.50 / 1M tokens
    output_cost_per_token: 0.0000015   # $1.50 / 1M tokens
    max_tokens: 128000
    max_output_tokens: 4096

  command-r-plus:
    mode: chat
    input_cost_per_token: 0.000003     # $3 / 1M tokens
    output_cost_per_token: 0.000015    # $15 / 1M tokens
    max_tokens: 128000
    max_output_tokens: 4096

  command-r7b-12-2024:
    mode: chat
    input_cost_per_token: 0.0000000375 # $0.0375 / 1M tokens
    output_cost_per_token: 0.00000015  # $0.15 / 1M tokens
    max_tokens: 128000

  command:
    mode: chat
    input_cost_per_token: 0.000000095  # $0.095 / 1M tokens
    output_cost_per_token: 0.00000019  # $0.19 / 1M tokens
    max_tokens: 4096

  command-light:
    mode: chat
    input_cost_per_token: 0.000000038  # $0.038 / 1M tokens
    output_cost_per_token: 0.000000038 # $0.038 / 1M tokens
    max_tokens: 4096

  # ============================================================================
  # Mistral Models
  # ============================================================================
  mistral-large-latest:
    mode: chat
    input_cost_per_token: 0.000002     # $2 / 1M tokens
    output_cost_per_token: 0.000006    # $6 / 1M tokens
    max_tokens: 128000
    max_output_tokens: 8192

  mistral-large-2411:
    mode: chat
    input_cost_per_token: 0.000002
    output_cost_per_token: 0.000006
    max_tokens: 128000
    max_output_tokens: 8192

  mistral-medium:
    mode: chat
    input_cost_per_token: 0.0000027    # $2.70 / 1M tokens
    output_cost_per_token: 0.0000081   # $8.10 / 1M tokens
    max_tokens: 32000
    max_output_tokens: 8192

  mistral-small:
    mode: chat
    input_cost_per_token: 0.000002
    output_cost_per_token: 0.000006
    max_tokens: 32000
    max_output_tokens: 8192

  mistral-tiny:
    mode: chat
    input_cost_per_token: 0.00000025   # $0.25 / 1M tokens
    output_cost_per_token: 0.00000025  # $0.25 / 1M tokens
    max_tokens: 32000

  pixtral-large-latest:
    mode: chat
    input_cost_per_token: 0.000002
    output_cost_per_token: 0.000006
    max_tokens: 128000

  pixtral-12b-2409:
    mode: chat
    input_cost_per_token: 0.00000022   # $0.22 / 1M tokens
    output_cost_per_token: 0.00000022  # $0.22 / 1M tokens
    max_tokens: 128000

  open-mistral-nemo:
    mode: chat
    input_cost_per_token: 0.00000015   # $0.15 / 1M tokens
    output_cost_per_token: 0.00000015  # $0.15 / 1M tokens
    max_tokens: 128000

  codestral-latest:
    mode: chat
    input_cost_per_token: 0.0000003    # $0.30 / 1M tokens
    output_cost_per_token: 0.0000009   # $0.90 / 1M tokens
    max_tokens: 32000

  # ============================================================================
  # Groq Models
  # ============================================================================
  llama-3.3-70b-versatile:
    mode: chat
    input_cost_per_token: 0.00000059   # $0.59 / 1M tokens
    output_cost_per_token: 0.00000079  # $0.79 / 1M tokens
    max_tokens: 128000
    max_output_tokens: 32768

  llama-3.1-70b-versatile:
    mode: chat
    input_cost_per_token: 0.00000059
    output_cost_per_token: 0.00000079
    max_tokens: 128000
    max_output_tokens: 8192

  llama-3.1-8b-instant:
    mode: chat
    input_cost_per_token: 0.000000059  # $0.059 / 1M tokens
    output_cost_per_token: 0.000000079 # $0.079 / 1M tokens
    max_tokens: 128000
    max_output_tokens: 8192

  llama3-70b-8192:
    mode: chat
    input_cost_per_token: 0.00000059
    output_cost_per_token: 0.00000079
    max_tokens: 8192

  llama3-8b-8192:
    mode: chat
    input_cost_per_token: 0.00000005   # $0.05 / 1M tokens
    output_cost_per_token: 0.00000008  # $0.08 / 1M tokens
    max_tokens: 8192

  llama-3.2-1b-preview:
    mode: chat
    input_cost_per_token: 0.00000004   # $0.04 / 1M tokens
    output_cost_per_token: 0.00000004  # $0.04 / 1M tokens
    max_tokens: 128000

  llama-3.2-3b-preview:
    mode: chat
    input_cost_per_token: 0.00000006   # $0.06 / 1M tokens
    output_cost_per_token: 0.00000006  # $0.06 / 1M tokens
    max_tokens: 128000

  llama-3.2-11b-vision-preview:
    mode: chat
    input_cost_per_token: 0.00000018   # $0.18 / 1M tokens
    output_cost_per_token: 0.00000018  # $0.18 / 1M tokens
    max_tokens: 128000

  llama-3.2-90b-vision-preview:
    mode: chat
    input_cost_per_token: 0.0000009    # $0.90 / 1M tokens
    output_cost_per_token: 0.0000009   # $0.90 / 1M tokens
    max_tokens: 128000

  mixtral-8x7b-32768:
    mode: chat
    input_cost_per_token: 0.00000024   # $0.24 / 1M tokens
    output_cost_per_token: 0.00000024  # $0.24 / 1M tokens
    max_tokens: 32768

  gemma2-9b-it:
    mode: chat
    input_cost_per_token: 0.0000002    # $0.20 / 1M tokens
    output_cost_per_token: 0.0000002   # $0.20 / 1M tokens
    max_tokens: 8192

  gemma-7b-it:
    mode: chat
    input_cost_per_token: 0.00000007   # $0.07 / 1M tokens
    output_cost_per_token: 0.00000007  # $0.07 / 1M tokens
    max_tokens: 8192

  # ============================================================================
  # Google Gemini Models
  # ============================================================================
  gemini-1.5-pro:
    mode: chat
    input_cost_per_token: 0.00000125   # $1.25 / 1M tokens (up to 128k)
    output_cost_per_token: 0.000005    # $5 / 1M tokens
    max_tokens: 2000000
    max_output_tokens: 8192

  gemini-1.5-pro-latest:
    mode: chat
    input_cost_per_token: 0.00000125
    output_cost_per_token: 0.000005
    max_tokens: 2000000
    max_output_tokens: 8192

  gemini-1.5-flash:
    mode: chat
    input_cost_per_token: 0.000000075  # $0.075 / 1M tokens
    output_cost_per_token: 0.0000003   # $0.30 / 1M tokens
    max_tokens: 1000000
    max_output_tokens: 8192

  gemini-1.5-flash-latest:
    mode: chat
    input_cost_per_token: 0.000000075
    output_cost_per_token: 0.0000003
    max_tokens: 1000000
    max_output_tokens: 8192

  gemini-1.0-pro:
    mode: chat
    input_cost_per_token: 0.0000005    # $0.50 / 1M tokens
    output_cost_per_token: 0.0000015   # $1.50 / 1M tokens
    max_tokens: 128000
    max_output_tokens: 2048

  # ============================================================================
  # Azure OpenAI (same pricing as OpenAI)
  # ============================================================================
  azure/gpt-4o:
    mode: chat
    input_cost_per_token: 0.0000025
    output_cost_per_token: 0.00001
    max_tokens: 128000
    max_output_tokens: 16384

  azure/gpt-4:
    mode: chat
    input_cost_per_token: 0.00003
    output_cost_per_token: 0.00006
    max_tokens: 8192
    max_output_tokens: 8192

  azure/gpt-35-turbo:
    mode: chat
    input_cost_per_token: 0.0000005
    output_cost_per_token: 0.0000015
    max_tokens: 16385
    max_output_tokens: 4096

  # ============================================================================
  # Embedding Models
  # ============================================================================
  text-embedding-3-small:
    mode: embedding
    input_cost_per_token: 0.00000002   # $0.02 / 1M tokens
    max_tokens: 8191

  text-embedding-3-large:
    mode: embedding
    input_cost_per_token: 0.00000013   # $0.13 / 1M tokens
    max_tokens: 8191

  text-embedding-ada-002:
    mode: embedding
    input_cost_per_token: 0.0000001    # $0.10 / 1M tokens
    max_tokens: 8191

  embed-english-v3.0:
    mode: embedding
    input_cost_per_token: 0.0000001    # Cohere
    max_tokens: 512

  embed-multilingual-v3.0:
    mode: embedding
    input_cost_per_token: 0.0000001    # Cohere
    max_tokens: 512

  # ============================================================================
  # Image Generation Models
  # ============================================================================
  dall-e-3:
    mode: image_generation
    image_sizes:
      "1024x1024": 0.040               # $0.040 per image
      "1024x1792": 0.080               # $0.080 per image
      "1792x1024": 0.080               # $0.080 per image
    quality_pricing:
      standard: 1.0
      hd: 2.0                          # 2x multiplier for HD

  dall-e-2:
    mode: image_generation
    image_sizes:
      "1024x1024": 0.020
      "512x512": 0.018
      "256x256": 0.016

  # ============================================================================
  # Audio TTS Models (Text-to-Speech)
  # ============================================================================
  tts-1:
    mode: audio_speech
    audio_cost_per_character: 0.000015 # $0.015 / 1K characters

  tts-1-hd:
    mode: audio_speech
    audio_cost_per_character: 0.000030 # $0.030 / 1K characters

  tts-1-1106:
    mode: audio_speech
    audio_cost_per_character: 0.000015

  tts-1-hd-1106:
    mode: audio_speech
    audio_cost_per_character: 0.000030

  # ============================================================================
  # Audio STT Models (Speech-to-Text)
  # ============================================================================
  whisper-1:
    mode: audio_transcription
    audio_cost_per_minute: 0.006       # $0.006 / minute

  whisper-large-v3:
    mode: audio_transcription
    audio_cost_per_minute: 0.004       # Groq pricing

  # ============================================================================
  # Moderation Models (usually free)
  # ============================================================================
  text-moderation-latest:
    mode: moderation
    input_cost_per_token: 0            # Free

  text-moderation-stable:
    mode: moderation
    input_cost_per_token: 0            # Free

  text-moderation-007:
    mode: moderation
    input_cost_per_token: 0            # Free

  omni-moderation-latest:
    mode: moderation
    input_cost_per_token: 0            # Free

  # ============================================================================
  # Rerank Models
  # ============================================================================
  cohere-rerank-v3-english:
    mode: rerank
    rerank_cost_per_search: 0.002      # $0.002 / search

  cohere-rerank-v3-multilingual:
    mode: rerank
    rerank_cost_per_search: 0.002      # $0.002 / search

  rerank-english-v3.0:
    mode: rerank
    rerank_cost_per_search: 0.002

  rerank-multilingual-v3.0:
    mode: rerank
    rerank_cost_per_search: 0.002

  # ============================================================================
  # Batch Models (50% discount)
  # ============================================================================
  gpt-4o-batch:
    mode: batch
    base_model: gpt-4o
    batch_discount_percent: 50.0
    input_cost_per_token: 0.00000125   # 50% of 0.0000025
    output_cost_per_token: 0.000005    # 50% of 0.00001
    max_tokens: 128000

  gpt-4o-mini-batch:
    mode: batch
    base_model: gpt-4o-mini
    batch_discount_percent: 50.0
    input_cost_per_token: 0.000000075  # 50% of 0.00000015
    output_cost_per_token: 0.0000003   # 50% of 0.0000006
    max_tokens: 128000

  # ============================================================================
  # Additional Open Source Models (via various providers)
  # ============================================================================
  # DeepSeek
  deepseek-chat:
    mode: chat
    input_cost_per_token: 0.00000014   # $0.14 / 1M tokens
    output_cost_per_token: 0.00000028  # $0.28 / 1M tokens
    max_tokens: 64000

  deepseek-coder:
    mode: chat
    input_cost_per_token: 0.00000014
    output_cost_per_token: 0.00000028
    max_tokens: 64000

  # Perplexity
  llama-3.1-sonar-small-128k-online:
    mode: chat
    input_cost_per_token: 0.0000002    # $0.20 / 1M tokens
    output_cost_per_token: 0.0000002   # $0.20 / 1M tokens
    max_tokens: 128000

  llama-3.1-sonar-large-128k-online:
    mode: chat
    input_cost_per_token: 0.000001     # $1 / 1M tokens
    output_cost_per_token: 0.000001    # $1 / 1M tokens
    max_tokens: 128000

  # AI21
  jamba-1.5-mini:
    mode: chat
    input_cost_per_token: 0.0000002    # $0.20 / 1M tokens
    output_cost_per_token: 0.0000004   # $0.40 / 1M tokens
    max_tokens: 256000

  jamba-1.5-large:
    mode: chat
    input_cost_per_token: 0.000002     # $2 / 1M tokens
    output_cost_per_token: 0.000008    # $8 / 1M tokens
    max_tokens: 256000


  # ============================================================================
  # Self-Hosted Providers (vLLM, Ollama)
  # These are self-hosted, so cost is $0 (users pay for their own infrastructure)
  # ============================================================================
  
  # vLLM Models (self-hosted)
  meta-llama/Llama-2-7b-chat-hf:
    mode: chat
    input_cost_per_token: 0            # Self-hosted = $0
    output_cost_per_token: 0           # Self-hosted = $0
    max_tokens: 4096

  meta-llama/Llama-2-13b-chat-hf:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 4096

  meta-llama/Llama-2-70b-chat-hf:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 4096

  meta-llama/Llama-3-8b-chat-hf:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 8192

  meta-llama/Llama-3-70b-chat-hf:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 8192

  mistralai/Mistral-7B-Instruct-v0.1:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 8192

  mistralai/Mistral-7B-Instruct-v0.2:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 32768

  mistralai/Mixtral-8x7B-Instruct-v0.1:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 32768

  # Ollama Models (self-hosted)
  # These use simplified model names (without the full path)
  llama3:
    mode: chat
    input_cost_per_token: 0            # Self-hosted = $0
    output_cost_per_token: 0           # Self-hosted = $0
    max_tokens: 8192

  llama3:8b:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 8192

  llama3:70b:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 8192

  llama3.1:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 131072

  llama2:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 4096

  mistral:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 32768

  mixtral:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 32768

  phi3:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 128000

  gemma:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 8192

  gemma2:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 8192

  qwen:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 32768

  codellama:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 16384

  deepseek-coder:
    mode: chat
    input_cost_per_token: 0
    output_cost_per_token: 0
    max_tokens: 16384
