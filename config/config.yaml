# ProxyLLM Configuration Example

# Model list - defines which models are available
model_list:
  # OpenAI models
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      tpm: 100000  # Tokens per minute limit
      rpm: 1000    # Requests per minute limit
  
  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
      tpm: 200000
      rpm: 2000
  
  # Anthropic models
  - model_name: claude-3-sonnet
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
      tpm: 50000
      rpm: 500
  
  - model_name: claude-3-haiku
    litellm_params:
      model: anthropic/claude-3-haiku-20240307
      api_key: os.environ/ANTHROPIC_API_KEY
      tpm: 100000
      rpm: 1000

# Router settings
router_settings:
  routing_strategy: latency-based  # simple-shuffle, least-busy, latency-based, cost-based
  num_retries: 3
  timeout: 120
  enable_cooldowns: true
  cooldown_time: 60
  cooldown_failure_threshold: 3
  fallbacks:
    - gpt-4o: [claude-3-sonnet, gpt-4o-mini]
    - claude-3-sonnet: [gpt-4o, claude-3-haiku]

# Proxy server settings
litellm_settings:
  drop_params: true  # Ignore unsupported parameters
  set_verbose: false
  
  # Caching
  cache: true
  cache_params:
    type: redis
    host: localhost
    port: 6379
    ttl: 3600

# General settings
general_settings:
  # Master key for admin operations
  master_key: os.environ/PROXYLLM_MASTER_KEY
  
  # Database
  database_url: os.environ/DATABASE_URL
  
  # Logging
  log_level: INFO
  
  # Allowed IPs (empty = allow all)
  allowed_ips: []
  
  # SSL
  ssl_certfile: null
  ssl_keyfile: null
  
  # Alerting
  alerting:
    - webhook
  alert_webhook_url: null
  
  # Request limits
  max_requests_per_minute: 10000
  max_tokens_per_minute: 1000000
